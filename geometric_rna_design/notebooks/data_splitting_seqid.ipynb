{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split Creation\n",
    "\n",
    "This notebook creates data splits used to evaluate gRNAde on biologically dissimilar clusters of RNAs.\n",
    "\n",
    "**Workflow:**\n",
    "1. Cluster RNA sample sequences into groups based on: \n",
    "    - Sequence identity -- CD-HIT (Fu et al., 2012) with identity threshold of 90%.\n",
    "    - Structural similarity -- US-align with similarity threshold 0.45 (TODO).\n",
    "2. Order the clusters based on some metric:\n",
    "    - Avg. of intra-sequence avg. RMSD among available structures\n",
    "    - Avg. of intra-sequence number of structures available\n",
    "3. Training, validation, and test splits become progressively harder.\n",
    "    - Top 100 samples from clusters with highest metric -- test set.\n",
    "    - Next 100 samples from clusters with highest metric -- validation set.\n",
    "    - All remaining samples -- training set.\n",
    "    - For clusters with >20 samples within them -- training set.\n",
    "    - Very large (> 1000 nts) or very small (< 10nts) RNAs -- training set.\n",
    "4. If any samples were not assigned clusters, append them to the training set.\n",
    "\n",
    "Note that we separate very large RNA samples (> 1000 nts) from clustering and directly add these to the training set, as it is unlikely that we want to redesign very large RNAs. Likewise for very short RNA samples (< 10 nts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dnori/miniconda3/envs/RF2NA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, InsetPosition, mark_inset\n",
    "import seaborn as sns\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "from src.data_utils import get_avg_rmsds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters_sequence_identity(\n",
    "        input_sequences, \n",
    "        identity_threshold = 0.9,\n",
    "        word_size = 2,\n",
    "        input_file = \"input\",\n",
    "        output_file = \"output\"\n",
    "    ):\n",
    "    # https://manpages.ubuntu.com/manpages/impish/man1/cd-hit-est.1.html\n",
    "        \n",
    "    # Write input sequences to the temporary input file\n",
    "    SeqIO.write(input_sequences, input_file, \"fasta\")\n",
    "\n",
    "    # Run CD-HIT-EST\n",
    "    cmd = [\n",
    "        \"cd-hit-est\",\n",
    "        \"-i\", input_file,\n",
    "        \"-o\", output_file,\n",
    "        \"-c\", str(identity_threshold), # Sequence identity threshold (e.g., 90%)\n",
    "        \"-n\", str(word_size)          # Word size for sequence comparisson, larger is better (default: 2)\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "    # Read clustered sequences from the temporary output file\n",
    "    clustered_sequences = list(SeqIO.parse(output_file, \"fasta\"))\n",
    "\n",
    "    # Process the clustering output\n",
    "    seq_idx_to_cluster = {}\n",
    "    with open(output_file + \".clstr\", \"r\") as f:\n",
    "        current_cluster = None\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                current_cluster = int(line.strip().split(\" \")[1])\n",
    "            else:\n",
    "                sequence_id = int(line.split(\">\")[1].split(\"...\")[0])\n",
    "                seq_idx_to_cluster[sequence_id] = current_cluster\n",
    "\n",
    "    # Delete temporary files\n",
    "    os.remove(input_file)\n",
    "    os.remove(output_file)\n",
    "    os.remove(output_file + \".clstr\")\n",
    "\n",
    "    return clustered_sequences, seq_idx_to_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data list\n",
    "data_list = torch.load(os.path.join(\"../data/\", \"processed.pt\"))\n",
    "print(len(data_list))\n",
    "\n",
    "# List of sample sequences (used to create .fasta input file)\n",
    "seq_list = []\n",
    "for idx, data in enumerate(data_list):\n",
    "    seq = data[\"seq\"]\n",
    "    seq_list.append(SeqRecord(Seq(seq), id=str(idx)))  # the ID for each sequence is its index in data_list\n",
    "\n",
    "# List of intra-sequence avg. RMSDs\n",
    "rmsd_list = get_avg_rmsds(data_list)\n",
    "\n",
    "# List of number of structures per sequence\n",
    "count_list = [len(data[\"coords_list\"]) for data in data_list]\n",
    "\n",
    "assert len(data_list) == len(seq_list) == len(rmsd_list) == len(count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111 16 124\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/home/dnori/rna-design/src/data/rf2na_split_dataset.pickle\", 'rb') as handle:\n",
    "    dataset = pickle.load(handle)\n",
    "    train_dict = dataset[\"train\"]\n",
    "    test_dict = dataset[\"test\"]\n",
    "    val_dict = dataset[\"val\"]\n",
    "    print(len(train_dict), len(test_dict), len(val_dict))\n",
    "\n",
    "seq_list = []\n",
    "pdb_id_list = []\n",
    "idx = 0\n",
    "idx_to_pdb = {}\n",
    "for dataset in [train_dict, val_dict, test_dict]:\n",
    "    for k,v in dataset.items():\n",
    "        seq = v[\"rna_seq\"]\n",
    "        seq_list.append(SeqRecord(Seq(seq), id=str(idx)))\n",
    "        pdb_id_list.append(k)\n",
    "        idx_to_pdb[str(idx)] = k\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Program: CD-HIT, V4.8.1 (+OpenMP), May 15 2023, 22:49:31\n",
      "Command: cd-hit-est -i input -o output -c 0.9 -n 2\n",
      "\n",
      "Started: Fri Jan 12 12:04:01 2024\n",
      "================================================================\n",
      "                            Output                              \n",
      "----------------------------------------------------------------\n",
      "Your word length is 2, using 5 may be faster!\n",
      "total seq: 1006\n",
      "longest and shortest : 242 and 11\n",
      "Total letters: 44876\n",
      "Sequences have been sorted\n",
      "\n",
      "Approximated minimal memory consumption:\n",
      "Sequence        : 0M\n",
      "Buffer          : 1 X 17M = 17M\n",
      "Table           : 1 X 0M = 0M\n",
      "Miscellaneous   : 0M\n",
      "Total           : 17M\n",
      "\n",
      "Table limit with the given memory limit:\n",
      "Max number of representatives: 4000000\n",
      "Max number of word counting entries: 97773433\n",
      "\n",
      "comparing sequences from          0  to       1006\n",
      ".\n",
      "     1006  finished        405  clusters\n",
      "\n",
      "Approximated maximum memory consumption: 17M\n",
      "writing new database\n",
      "writing clustering information\n",
      "program completed !\n",
      "\n",
      "Total CPU time 0.12\n"
     ]
    }
   ],
   "source": [
    "# Cluster at 80% sequence identity (lowest currently possible)\n",
    "clustered_sequences, seq_idx_to_cluster = create_clusters_sequence_identity(seq_list, identity_threshold=0.8, word_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for idx, cluster in seq_idx_to_cluster.items():\n",
    "    result[idx_to_pdb[str(idx)]] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/dnori/rna-design/src/data/rna_seq_clusters.pkl\", 'wb') as handle:\n",
    "    pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006\n"
     ]
    }
   ],
   "source": [
    "print(len(result.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: it seems very short sequences (<10nt) are not being clustered.\n",
    "# These will be added to the training set after initial splitting.\n",
    "try:\n",
    "    # Why does this fail? Guess: sequences are too short?\n",
    "    assert len(seq_idx_to_cluster.keys()) == len(seq_list)\n",
    "except:\n",
    "    # Which sequence indices are not clustered? What are their corresponding sequences?\n",
    "    idx_not_clustered = list(set(list(range(len(data_list)))) - set(seq_idx_to_cluster.keys()))\n",
    "    print(\"Number of missing indices after clustering: \", len(idx_not_clustered))\n",
    "    \n",
    "    seq_lens = []\n",
    "    for idx in idx_not_clustered:\n",
    "        seq_lens.append(len(data_list[idx][\"seq\"]))\n",
    "    print(\"Sequence lengths for missing indices:\")\n",
    "    print(f\"    Distribution: {np.mean(seq_lens)} +- {np.std(seq_lens)}\")\n",
    "    print(f\"    Max: {np.max(seq_lens)}, Min: {np.min(seq_lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_idx_to_cluster: (index in data_list: cluster ID)\n",
    "# (NEW) cluster_to_seq_idx_list: (cluster ID: list of indices in data_list)\n",
    "cluster_to_seq_idx_list = {}\n",
    "for seq_idx, cluster in seq_idx_to_cluster.items():\n",
    "    # Sanity check to filter very large or very small RNAs\n",
    "    if len(seq_list[seq_idx]) > 1000 or len(seq_list[seq_idx]) < 10 and seq_idx not in idx_not_clustered:\n",
    "        idx_not_clustered.append(seq_idx)\n",
    "        # print(f\"Pruned idx {seq_idx} of length {len(seq_list[seq_idx])}.\")\n",
    "    else:\n",
    "        if cluster in cluster_to_seq_idx_list.keys():\n",
    "            cluster_to_seq_idx_list[cluster].append(seq_idx)\n",
    "        else:\n",
    "            cluster_to_seq_idx_list[cluster] = [seq_idx]\n",
    "print(\"Number of unassigned indices (not clustered + too large + too small): \", len(idx_not_clustered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster sizes: number of sequences in each cluster\n",
    "cluster_ids = list(cluster_to_seq_idx_list.keys())\n",
    "cluster_sizes = [len(list) for list in cluster_to_seq_idx_list.values()]\n",
    "\n",
    "# Number of structures in each cluster (total and intra-sequence avg.)\n",
    "total_structs_list = []\n",
    "avg_structs_list = []\n",
    "avg_rmsds_list = []\n",
    "avg_seq_len_list = []\n",
    "for cluster, seq_idx_list in cluster_to_seq_idx_list.items():\n",
    "    count = []\n",
    "    rmsds = []\n",
    "    lens = []\n",
    "    for seq_idx in seq_idx_list:\n",
    "        count.append(count_list[seq_idx])\n",
    "        rmsds.append(rmsd_list[seq_idx])\n",
    "        lens.append(len(seq_list[seq_idx]))\n",
    "    total_structs_list.append(np.sum(count))\n",
    "    avg_structs_list.append(np.mean(count))\n",
    "    avg_rmsds_list.append(np.mean(rmsds))\n",
    "    avg_seq_len_list.append(np.mean(lens))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Cluster ID': cluster_ids,\n",
    "    'Cluster size': cluster_sizes,\n",
    "    'Total no. structures': total_structs_list,\n",
    "    'Avg. sequence length': avg_seq_len_list,\n",
    "    'Avg. intra-sequence no. structures': avg_structs_list,\n",
    "    'Avg. intra-sequence avg. RMSD': avg_rmsds_list,\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSD Split\n",
    "\n",
    "# Zip the two lists together\n",
    "zipped = zip(cluster_ids, avg_rmsds_list)\n",
    "# Sort the zipped list based on the values (descending order, highest first)\n",
    "sorted_zipped = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
    "# Unzip the sorted list back into two separate lists\n",
    "sorted_cluster_ids, sorted_avg_rmsds_list = zip(*sorted_zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx_list = []\n",
    "val_idx_list = []\n",
    "train_idx_list = []\n",
    "\n",
    "for cluster in sorted_cluster_ids:\n",
    "    seq_idx_list = cluster_to_seq_idx_list[cluster]\n",
    "    cluster_size = len(seq_idx_list)\n",
    "\n",
    "    # Test set\n",
    "    if len(test_idx_list) < 100 and cluster_size < 25:\n",
    "        test_idx_list += seq_idx_list\n",
    "    \n",
    "    # Validation set\n",
    "    elif len(val_idx_list) < 100 and cluster_size < 25:\n",
    "        val_idx_list += seq_idx_list\n",
    "    \n",
    "    # Training set\n",
    "    else:\n",
    "        train_idx_list += seq_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all the sequences that were not assigned any clusters into the training set\n",
    "try:\n",
    "    assert len(test_idx_list) + len(val_idx_list) + len(train_idx_list) == len(data_list)\n",
    "except:\n",
    "    train_idx_list += idx_not_clustered\n",
    "    assert len(test_idx_list) + len(val_idx_list) + len(train_idx_list) == len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((train_idx_list, val_idx_list, test_idx_list), \"../data/seqid_rmsd_split.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
